{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pandas import DataFrame as df\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9162, 7744)\n"
     ]
    }
   ],
   "source": [
    "otu_notrim = pd.read_csv('../data/AG_new/03-otus/03-otus/notrim/gg-13_8-97-percent/otu_table_notrim.txt', sep = '\\t', index_col = 0)\n",
    "print(otu_notrim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35511, 19491)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otu_notrim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Throw samples with less than 5000 reads\n",
    "sample_sums = np.sum(otu_notrim, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sample_sums > 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35511, 15475)\n"
     ]
    }
   ],
   "source": [
    "otu_filter = otu_notrim.loc[:, np.array(sample_sums > 5000)]\n",
    "print(otu_filter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35275, 15475)\n"
     ]
    }
   ],
   "source": [
    "taxa_sums = np.sum(otu_filter, axis = 1)\n",
    "otu_filter = otu_filter.loc[np.array(taxa_sums > 0), :]\n",
    "print(otu_filter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ctata\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (0,81) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\ctata\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Read in map file so we can decide on samples to include\n",
    "mapping = pd.read_csv(\"../data/AG_new/AG_mapping.txt\", sep = \"\\t\", index_col=0)\n",
    "map_clean = mapping.loc[otu_filter.columns.values] #Keep samples if present in otu\n",
    "map_clean = map_clean.reindex(sorted(map_clean.index.values))\n",
    "otu_clean = otu_filter.reindex(sorted(otu_filter.columns.values), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15475, 527)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35275, 15475)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(map_clean.shape)\n",
    "otu_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide on what samples should be included in the embedding calculations, and name files accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15475"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select all body sites\n",
    "\n",
    "#otu_table_name = \"data/AG_new/otu_filtered_AG_02perc_allbodysites.csv\"\n",
    "#otu_train_table_name = \"data/AG_new/otu_filtered_train_AG_02perc_allbodysites.csv\"\n",
    "#otu_test_table_name = \"data/AG_new/otu_filtered_test_AG_02perc_allbodysites.csv\"\n",
    "#test_sample_file = \"data/AG_new/AG_test_samples_allbodysites.obj\"\n",
    "\n",
    "np.sum([map_clean.index.values[i] == otu_clean.columns.values[i] for i in range(otu_clean.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "otu_table_name = \"../data/AG_new/feces/otu_filtered_AG_07perc_feces.csv\"\n",
    "otu_train_table_name = \"../data/AG_new/feces/otu_filtered_train_AG_07perc_feces.csv\"\n",
    "otu_test_table_name = \"../data/AG_new/feces/otu_filtered_test_AG_07perc_feces.csv\"\n",
    "test_sample_file = \"../data/AG_new/feces/AG_test_samples_feces.obj\"\n",
    "glove_output_file = \"../data/AG_new/feces/glove_input_07perc_feces_sampledata.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only feces samples\n",
    "otu_use = otu_clean.loc[:, map_clean[\"BODY_SITE\"] == \"UBERON:feces\"]\n",
    "map_use = map_clean.loc[map_clean[\"BODY_SITE\"] == \"UBERON:feces\", :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanBySampleData(otu_clean, map_clean, cat_criteria, number_criteria):\n",
    "    keep = [True] * map_clean.shape[0]\n",
    "    print(\"Samples originally: \" + str(sum(keep)))\n",
    "    for criteria in cat_criteria:\n",
    "        #Keep sample if it has desired metadata available\n",
    "        keep_tmp = [( (i != \"Unknown\") and (i != \"Unspecified\") and (i!=\"other\" ) and (i != \"unspecified\") and (isinstance(i, str)) )  for i in map_clean[criteria]] \n",
    "\n",
    "        keep = [(i and j) for (i,j) in zip(keep, keep_tmp)]\n",
    "    print(\"Samples after categorical filter: \" + str(sum(keep)))\n",
    "\n",
    "    for criteria in number_criteria:\n",
    "        keep_tmp = [hf.is_number(i) for i in map_clean[criteria]]\n",
    "        keep = [i and j for (i,j) in zip(keep, keep_tmp)] \n",
    "    print(\"Samples after numerical filter: \" + str(sum(keep)))\n",
    "\n",
    "\n",
    "    otu_keep = otu_clean.loc[:, keep]\n",
    "    map_keep = map_clean.loc[keep, cat_criteria + number_criteria]   \n",
    "    otu_keep = otu_keep.T\n",
    "    return(otu_keep, map_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples originally: 9132\n",
      "Samples after categorical filter: 5980\n",
      "Samples after numerical filter: 5980\n"
     ]
    }
   ],
   "source": [
    "number_criteria = []\n",
    "cat_criteria = [\"IBD\", \"EXERCISE_FREQUENCY\", \"SEX\", \"ONE_LITER_OF_WATER_A_DAY_FREQUENCY\", \n",
    "        \"SEAFOOD_FREQUENCY\", \"PROBIOTIC_FREQUENCY\", \"OLIVE_OIL\", \"FRUIT_FREQUENCY\", \n",
    "         \"SLEEP_DURATION\", \"SUGAR_SWEETENED_DRINK_FREQUENCY\", \"MILK_CHEESE_FREQUENCY\",\n",
    "         \"RED_MEAT_FREQUENCY\",\"MEAT_EGGS_FREQUENCY\", \"VEGETABLE_FREQUENCY\"]\n",
    "\n",
    "otu_use, map_use = cleanBySampleData(otu_use, map_use, cat_criteria, number_criteria)\n",
    "otu_use.index = otu_use.index.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only keep taxa present in greater than 4.186 samples\n",
      "We will keep 9230 taxa\n"
     ]
    }
   ],
   "source": [
    "thresh = 0.0007 * otu_use.shape[0] # at least 1 % of samples (10 in this case)\n",
    "print(\"Only keep taxa present in greater than \" + str(thresh) + \" samples\")\n",
    "binary = otu_use > 0\n",
    "keep = binary.sum(axis = 0) >= thresh# Should be ntaxa\n",
    "binary = binary.loc[:, keep]\n",
    "print(\"We will keep \" + str(keep.sum()) +\" taxa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5980, 9230)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(test_sample_file, \"rb\")\n",
    "test_samples = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5068, 9230)\n",
      "(5068, 35275)\n",
      "(983, 35275)\n",
      "(5068, 14)\n",
      "(983, 14)\n"
     ]
    }
   ],
   "source": [
    "#Break out train and test\n",
    "keep = [(i in otu_use.index.values) for i in test_samples]\n",
    "test_samples = test_samples[keep]\n",
    "\n",
    "binary_train = binary.loc[[not(i in test_samples) for i in binary.index.values ], :]\n",
    "otu_train = otu_use.loc[[not(i in test_samples) for i in binary.index.values ], :]\n",
    "otu_test = otu_use.loc[test_samples, :]\n",
    "map_train = map_use.loc[otu_train.index.values, :]\n",
    "map_test = map_use.loc[test_samples, :]\n",
    "print(binary_train.shape)\n",
    "print(otu_train.shape)\n",
    "print(otu_test.shape)\n",
    "print(map_train.shape)\n",
    "print(map_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXERCISE_FREQUENCY\n",
      "ONE_LITER_OF_WATER_A_DAY_FREQUENCY\n",
      "SEAFOOD_FREQUENCY\n",
      "PROBIOTIC_FREQUENCY\n",
      "OLIVE_OIL\n",
      "FRUIT_FREQUENCY\n",
      "SUGAR_SWEETENED_DRINK_FREQUENCY\n",
      "MILK_CHEESE_FREQUENCY\n",
      "RED_MEAT_FREQUENCY\n",
      "MEAT_EGGS_FREQUENCY\n",
      "VEGETABLE_FREQUENCY\n",
      "SLEEP_DURATION\n",
      "SEX\n",
      "IBD\n"
     ]
    }
   ],
   "source": [
    "#Change sample data to one-hot\n",
    "map_train_num, map_test_num = hf.makeMappingNumeric(map_train, map_test, number_criteria, cat_criteria)\n",
    "\n",
    "#Transform to one hot\n",
    "map_train_1hot = pd.concat([pd.get_dummies(map_train_num[col], prefix = col) for col in map_train_num], axis=1)\n",
    "map_test_1hot = pd.concat([pd.get_dummies(map_test_num[col], prefix = col) for col in map_test_num], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5068, 9230)\n",
      "(5068, 64)\n",
      "(5068, 9294)\n"
     ]
    }
   ],
   "source": [
    "concat = pd.concat([binary_train, map_train_1hot], axis = 1)\n",
    "print(binary_train.shape)\n",
    "print(map_train_1hot.shape)\n",
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                1081058,                  359105,\n",
       "                       1134664,                  311173,\n",
       "                       2700687,                  842284,\n",
       "                        546305,                 3450454,\n",
       "                       1909053,                  362389,\n",
       "       ...\n",
       "       'MEAT_EGGS_FREQUENCY_0', 'MEAT_EGGS_FREQUENCY_1',\n",
       "       'MEAT_EGGS_FREQUENCY_2', 'MEAT_EGGS_FREQUENCY_3',\n",
       "       'MEAT_EGGS_FREQUENCY_4', 'VEGETABLE_FREQUENCY_0',\n",
       "       'VEGETABLE_FREQUENCY_1', 'VEGETABLE_FREQUENCY_2',\n",
       "       'VEGETABLE_FREQUENCY_3', 'VEGETABLE_FREQUENCY_4'],\n",
       "      dtype='object', length=9294)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxa_share = concat.columns[concat.iloc[i,:] == True]\n",
    "concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text = []\n",
    "for i in range(concat.shape[0]):\n",
    "    #For every sample\n",
    "    taxa_share = concat.columns[concat.iloc[i,:] == True] #write names of every taxa in that sample\n",
    "    file_text.append(taxa_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of taxa in any given sample is: 1784\n",
      "Found at 1746\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for taxa_list in file_text:\n",
    "    lengths.append(len(taxa_list))\n",
    "max_val = max(lengths)\n",
    "max_ind = np.argmax(lengths)\n",
    "print(\"Max number of taxa in any given sample is: \" + str(max_val))\n",
    "print(\"Found at \" + str(max_ind))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([4480529, 4442508, 4409730, 4450194, 4467447], dtype='object')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_text[1][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Glove input file\n",
    "with open(glove_output_file, mode = 'w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter = \"\\t\", quoting = csv.QUOTE_NONE, escapechar = '')\n",
    "    for taxa_list in file_text:\n",
    "        writer.writerow(taxa_list)\n",
    "file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
